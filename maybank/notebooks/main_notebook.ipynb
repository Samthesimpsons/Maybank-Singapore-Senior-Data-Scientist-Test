{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.chdir(\"C:/Users/samue/Downloads/Maybank/\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_ID</th>\n",
       "      <th>C_AGE</th>\n",
       "      <th>C_EDU</th>\n",
       "      <th>C_HSE</th>\n",
       "      <th>PC</th>\n",
       "      <th>INCM_TYP</th>\n",
       "      <th>gn_occ</th>\n",
       "      <th>NUM_PRD</th>\n",
       "      <th>CASATD_CNT</th>\n",
       "      <th>MTHCASA</th>\n",
       "      <th>...</th>\n",
       "      <th>MAXUT</th>\n",
       "      <th>N_FUNDS</th>\n",
       "      <th>CC_AVE</th>\n",
       "      <th>MAX_MTH_TRN_AMT</th>\n",
       "      <th>MIN_MTH_TRN_AMT</th>\n",
       "      <th>AVG_TRN_AMT</th>\n",
       "      <th>ANN_TRN_AMT</th>\n",
       "      <th>ANN_N_TRX</th>\n",
       "      <th>CC_LMT</th>\n",
       "      <th>C_seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1443</td>\n",
       "      <td>65</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EXECUTIVE CONDOMINIUM</td>\n",
       "      <td>19250.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PMEB</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6896.91</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.233333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34500.0</td>\n",
       "      <td>AFFLUENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1559</td>\n",
       "      <td>86</td>\n",
       "      <td>O-Levels</td>\n",
       "      <td>PRIVATE CONDOMINIUM</td>\n",
       "      <td>99018.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PMEB</td>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51714.78</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>727.629167</td>\n",
       "      <td>8530.88</td>\n",
       "      <td>273.44</td>\n",
       "      <td>2296.713333</td>\n",
       "      <td>27560.56</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>AFFLUENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1913</td>\n",
       "      <td>69</td>\n",
       "      <td>A-Levels</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10155.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PMEB</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5420.09</td>\n",
       "      <td>...</td>\n",
       "      <td>59600.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>367.389167</td>\n",
       "      <td>523.35</td>\n",
       "      <td>122.13</td>\n",
       "      <td>283.580833</td>\n",
       "      <td>3402.97</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>AFFLUENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_ID  C_AGE     C_EDU                  C_HSE       PC  INCM_TYP gn_occ  \\\n",
       "0  1443     65   Masters  EXECUTIVE CONDOMINIUM  19250.0       6.0   PMEB   \n",
       "1  1559     86  O-Levels    PRIVATE CONDOMINIUM  99018.0       2.0   PMEB   \n",
       "2  1913     69  A-Levels                    NaN  10155.0       3.0   PMEB   \n",
       "\n",
       "   NUM_PRD  CASATD_CNT   MTHCASA  ...     MAXUT  N_FUNDS      CC_AVE  \\\n",
       "0        3         8.0   6896.91  ...       NaN      NaN   13.233333   \n",
       "1        4        13.0  51714.78  ...       NaN      NaN  727.629167   \n",
       "2        4         1.0   5420.09  ...  59600.88      1.0  367.389167   \n",
       "\n",
       "   MAX_MTH_TRN_AMT  MIN_MTH_TRN_AMT  AVG_TRN_AMT  ANN_TRN_AMT  ANN_N_TRX  \\\n",
       "0              NaN              NaN          NaN          NaN        NaN   \n",
       "1          8530.88           273.44  2296.713333     27560.56       88.0   \n",
       "2           523.35           122.13   283.580833      3402.97       78.0   \n",
       "\n",
       "    CC_LMT     C_seg  \n",
       "0  34500.0  AFFLUENT  \n",
       "1   4000.0  AFFLUENT  \n",
       "2   5000.0  AFFLUENT  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maybank.src.data_processing.data_loading import (\n",
    "    load_catalog,\n",
    "    load_raw_data,\n",
    "    load_meta_data,\n",
    ")\n",
    "\n",
    "catalog_config = load_catalog(\"maybank/conf/base/catalog.yaml\")\n",
    "\n",
    "raw_data = load_raw_data(catalog_config)\n",
    "\n",
    "raw_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID</td>\n",
       "      <td>Dummy customer ID</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_AGE</td>\n",
       "      <td>customer Age</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_EDU</td>\n",
       "      <td>customer Education</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature          Definition Remarks\n",
       "0    C_ID   Dummy customer ID     NaN\n",
       "1   C_AGE        customer Age     NaN\n",
       "2   C_EDU  customer Education     NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = load_meta_data(catalog_config)\n",
    "\n",
    "meta_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sweetviz for my exploratory data analysis (EDA) is an excellent choice as it offers a comprehensive suite of interactive visualizations and insights.\n",
    "\n",
    "With features like histogram charts, association analysis, counts, and detection of missing and zero values, Sweetviz provides everything needed for a thorough examination of the dataset.\n",
    "\n",
    "It's a convenient tool for quickly gaining valuable insights into the data distribution, correlations, and potential issues before delving deeper into analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please open `maybank/data/raw/EDA.html` to view the interactive html EDA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Downloads\\Maybank\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:01 -> (00:00 left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report maybank/data/raw/EDA.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "from maybank.src.data_processing.data_preprocessing import (\n",
    "    convert_c_seg_to_binary,\n",
    "    perform_eda_with_sweetviz,\n",
    ")\n",
    "\n",
    "raw_data = convert_c_seg_to_binary(raw_data)\n",
    "\n",
    "perform_eda_with_sweetviz(\n",
    "    raw_data, target_feat=\"C_seg\", html_file_path=\"maybank/data/raw/EDA.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputations for Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By delving into the metadata to understand the definition of each feature, we can make informed assumptions and devise the most suitable approach for handling missing values.\n",
    "\n",
    "This process enables us to tailor our imputation strategy based on the characteristics and significance of each feature, ensuring a more effective data treatment approach.\n",
    "\n",
    "\n",
    "| No. | Variable           | Missing % | Imputation Strategy                                | Explanation                                                                                                                                                             |\n",
    "|-----|--------------------|-----------|----------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1   | C_ID               | 0%        | No missing values                                  | -                                                                                                                                                                       |\n",
    "| 2   | C_AGE              | 0%        | No missing values                                  | -                                                                                                                                                                       |\n",
    "| 3   | C_EDU              | 58%       | Constant Imputation: \"Not Provided\"                | Since it is the customer education, missing values are imputed with \"Not Provided\", cannot use \"Others\" as it is already specified.                                     |\n",
    "| 4   | C_HSE              | 66%       | Constant Imputation: \"Not Provided\"                | Since it is the customer house type, missing values are imputed with \"Not Provided\".                                                                                    |\n",
    "| 5   | PC                 | <1%       | Zero Imputation                                    | Distribution is not skewed and only <1% missing. PC is postal code, hence does not make sense to use mean/mode imputation. Hence use zero imputation instead.           |\n",
    "| 6   | INCM_TYP           | 45%       | Mode Imputation: 2.0                               | Drop at 1, 7, and 8 bins. Impute with 2, assuming no income would be already be recorded under either 1 or 8 depending on which is the lowest income type bin.          |\n",
    "| 7   | gn_occ             | 1%        | Constant Imputation: \"Not Provided\"                | Since it is occupation, missing values are imputed with \"Not Provided\", cannot use \"Others\" as it is already specified.                                                 |\n",
    "| 8   | NUM_PRD            | 0%        | No missing values                                  | -                                                                                                                                                                       |\n",
    "| 9   | CASATD_CNT         | 38%       | Zero Imputation: 0                                 | Impute with 0, indicating no CASA or TD accounts, verified with below CASA and TD analysis.                                                                             |\n",
    "| 10  | MTHCASA            | 41%       | Zero Imputation: 0                                 | Impute with 0, indicating no CASA account.                                                                                                                              |\n",
    "| 11  | MAXCASA            | 41%       | Zero Imputation: 0                                 | Impute with 0, indicating no CASA account.                                                                                                                              |\n",
    "| 12  | MINCASA            | 41%       | Zero Imputation: 0                                 | Impute with 0, indicating no CASA account.                                                                                                                              |\n",
    "| 13  | OWN_CASA (NEW)     | -         | New Binary Column: 1 for owned, 0 for not owned    | Created a new binary column to differentiate not owning a CASA and an empty CASA account, since MTH/MAX/MINCASA already have 0s, which implies an empty CASA account.   |\n",
    "| 14  | DRvCR              | 55%       | Zero Imputation: 0                                 | Impute with 0, indicating either zero debit or absence of credit (0 division).                                                                                          |\n",
    "| 15  | MTHTD              | -         | Zero Imputation: 0                                 | Impute with 0, indicating no TD account                                                                                                                                 |\n",
    "| 16  | MAXTD              | -         | Zero Imputation: 0                                 | Impute with 0, indicating no TD account                                                                                                                                 |\n",
    "| 17  | OWN_TD (NEW)       | -         | New Binary Column: 1 for owned, 0 for not owned    | Created a new binary column, upon below CASA and TD analysis, it can be shown that indeed missing values in CASATD_CNT implies 0 CASA and TD accounts.                  |\n",
    "| 18  | Asset_value        | 0%        | No missing values                                  | -                                                                                                                                                                       |\n",
    "| 19  | HL_tag             | 96%       | Constant Imputation: 0                             | Supposed to be either 1 or 0 according to metadata, and only 1 exists at the moment. Impute with 0.                                                                     |\n",
    "| 20  | AL_tag             | 92%       | Constant Imputation: 0                             | Supposed to be either 1 or 0 according to metadata, and only 1 exists at the moment. Impute with 0.                                                                     |\n",
    "| 21  | pur_price_avg      | 92%       | Zero Imputation: 0                                 | Impute with 0, no 0s, hence should be safe assumption that the property purchase price is 0. Also _avg could imply average, hence if no property owned, then 0 division.|\n",
    "| 22  | UT_AVE             | 96%       | Zero Imputation: 0                                 | Impute with 0, indicating no UT transaction, no 0 exists and verified with below Unit Trust Analysis.                                                                   |\n",
    "| 23  | MAXUT              | 96%       | Zero Imputation: 0                                 | Impute with 0, indicating no UT transaction, no 0 exists and verified with below Unit Trust Analysis.                                                                   |\n",
    "| 24  | N_FUNDS            | 96%       | Zero Imputation: 0                                 | Impute with 0, indicating no funds owned, no 0 exists and verified with Unit Trust Analysis.                                                                            |\n",
    "| 25  | MAX_MTH_TRN_AMT    | 82%       | Zero Imputation: 0                                 | Impute with 0, verified with below Credit Card TRN Analysis, follow imputation of ANN_N_TRX.                                                                            |\n",
    "| 26  | MIN_MTH_TRN_AMT    | 82%       | Zero Imputation: 0                                 | Impute with 0, verified with below Credit Card TRN Analysis, follow imputation of ANN_N_TRX.                                                                            |\n",
    "| 27  | AVG_TRN_AMT        | 82%       | Zero Imputation: 0                                 | Impute with 0, verified with below Credit Card TRN Analysis, follow imputation of ANN_N_TRX.                                                                            |\n",
    "| 28  | ANN_TRN_AMT        | 82%       | Zero Imputation: 0                                 | Impute with 0, verified with below Credit Card TRN Analysis, follow imputation of ANN_N_TRX.                                                                            |\n",
    "| 29  | ANN_N_TRX          | 82%       | Zero Imputation: 0                                 | Impute with 0. 0 does not exist hence 82% either no credit transaction or no credit card owned.                                                                         |\n",
    "| 30  | CC_AVE             | 74%       | Zero Imputation: 0                                 | Impute with 0, indicating no credit card owned in the past, hence safe to impute with 0.                                                                                |\n",
    "| 31  | CC_LMT             | 28%       | Zero Imputation: 0                                 | Impute with 0. indicating no credit card owned at the moment, hence safe to impute with 0. Assumption still valid since 28% within the above 82% of rows 25-29.         |\n",
    "| 32  | OWN_CC (NEW)       | -         | New Binary Column: 1 for owned, 0 for not owned    | Created new binary column, to distinguish 0 credit limit due to bank assigning 0 limit and not owning a credit card hence 0 limit due to imputation.                    |\n",
    "| 32  | OWN_PREV_CC (NEW)  | -         | New Binary Column: 1 for owned, 0 for not owned    | Created new binary column, to distinguish 0 CC_AVE due to 0 CC_AVE and not owning a credit card in past hence 0 limit due to imputation.                                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASA and TD Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am identifying and validating indices where missing values in monthly, max, and min CASA and TD accounts imply zero accounts owned, ensuring consistency and accuracy in data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "casa_td_missing_indices = set(raw_data[raw_data[\"CASATD_CNT\"].isna()].index)\n",
    "missing_monthly_casa_indices = set(raw_data[raw_data[\"MTHCASA\"].isna()].index)\n",
    "missing_max_casa_indices = set(raw_data[raw_data[\"MAXCASA\"].isna()].index)\n",
    "missing_min_casa_indices = set(raw_data[raw_data[\"MINCASA\"].isna()].index)\n",
    "missing_monthly_td_indices = set(raw_data[raw_data[\"MTHTD\"].isna()].index)\n",
    "missing_max_td_indices = set(raw_data[raw_data[\"MAXTD\"].isna()].index)\n",
    "\n",
    "# Ensure missing monthly, max, and min CASA indices are the same\n",
    "assert missing_monthly_casa_indices == missing_max_casa_indices == missing_min_casa_indices\n",
    "\n",
    "# Ensure missing monthly and max TD indices are the same\n",
    "assert missing_monthly_td_indices == missing_max_td_indices\n",
    "\n",
    "# Check intersection of indices of missing monthly CASA and TD, which imply 0 CASA and TD accounts owned\n",
    "intersection_casa_td = missing_monthly_casa_indices.intersection(missing_monthly_td_indices)\n",
    "\n",
    "# Validate assumption that missing monthly CASA and TD data imply 0 CASA and TD accounts\n",
    "assert casa_td_missing_indices == intersection_casa_td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Trust Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am confirming that missing indices for average UT, maximum UT, and the number of UT funds are consistent, indicating zero funds owned. Therefore, it's safe to impute these missing values with 0, reflecting that the customer owns no funds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ut_ave_indices = set(raw_data[raw_data[\"UT_AVE\"].isna()].index)\n",
    "missing_max_ut_indices = set(raw_data[raw_data[\"MAXUT\"].isna()].index)\n",
    "missing_n_funds_indices = set(raw_data[raw_data[\"N_FUNDS\"].isna()].index)\n",
    "\n",
    "# Ensure missing indices for average UT, maximum UT, and number of UT funds are the same\n",
    "assert missing_ut_ave_indices == missing_max_ut_indices == missing_n_funds_indices\n",
    "\n",
    "# Safe to impute these missing values with 0, indicating the customer owns 0 funds\n",
    "# This implies no average UT and MAXUT values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit Card Trn Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am confirming that missing indices for maximum, minimum, average, and annual transaction amounts, as well as the annual number of transactions, are consistent. This suggests either no credit card ownership or no credit transactions for these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_max_monthly_trn_amt_indices = set(\n",
    "    raw_data[raw_data[\"MAX_MTH_TRN_AMT\"].isna()].index\n",
    ")\n",
    "missing_min_monthly_trn_amt_indices = set(\n",
    "    raw_data[raw_data[\"MIN_MTH_TRN_AMT\"].isna()].index\n",
    ")\n",
    "missing_avg_trn_amt_indices = set(raw_data[raw_data[\"AVG_TRN_AMT\"].isna()].index)\n",
    "missing_annual_trn_amt_indices = set(raw_data[raw_data[\"ANN_TRN_AMT\"].isna()].index)\n",
    "missing_annual_n_trx_indices = set(raw_data[raw_data[\"ANN_N_TRX\"].isna()].index)\n",
    "\n",
    "# Ensure missing indices for maximum, minimum, average, and annual transaction amount, \n",
    "# as well as annual number of transactions, are the same\n",
    "assert (\n",
    "    missing_max_monthly_trn_amt_indices\n",
    "    == missing_min_monthly_trn_amt_indices\n",
    "    == missing_avg_trn_amt_indices\n",
    "    == missing_annual_trn_amt_indices\n",
    "    == missing_annual_n_trx_indices\n",
    ")\n",
    "\n",
    "# Assume missing values imply either no credit card ownership or no credit transactions for these rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Impute missing data based on above analysis\n",
    "- Optimize efficiency by converting float64 to float32\n",
    "<!-- - Drop outliers using the z-score method.\n",
    "  - Using the z-score method with a threshold of 3 allows us to identify data points that are significantly distant from the mean in terms of standard deviations. This approach is particularly useful when dealing with heavily left-skewed features, as it provides a standardized way to detect and remove extreme values that might otherwise distort our analysis or modeling efforts. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maybank.src.data_processing.data_preprocessing import (\n",
    "    impute_missing_data,\n",
    "    convert_float64_to_float32,\n",
    "    drop_outliers,  \n",
    "    # This approach hasn't been utilized; it employs z-score outlier detection implementation. \n",
    "    # Other techniques, such as isolation forest, could be considered. \n",
    "    # Initially included for clustering modeling, which is sensitive to outliers.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_AGE</th>\n",
       "      <th>C_EDU</th>\n",
       "      <th>C_HSE</th>\n",
       "      <th>INCM_TYP</th>\n",
       "      <th>gn_occ</th>\n",
       "      <th>NUM_PRD</th>\n",
       "      <th>CASATD_CNT</th>\n",
       "      <th>MTHCASA</th>\n",
       "      <th>MAXCASA</th>\n",
       "      <th>MINCASA</th>\n",
       "      <th>...</th>\n",
       "      <th>CC_AVE</th>\n",
       "      <th>MAX_MTH_TRN_AMT</th>\n",
       "      <th>MIN_MTH_TRN_AMT</th>\n",
       "      <th>AVG_TRN_AMT</th>\n",
       "      <th>ANN_TRN_AMT</th>\n",
       "      <th>ANN_N_TRX</th>\n",
       "      <th>CC_LMT</th>\n",
       "      <th>C_seg</th>\n",
       "      <th>CC_AVE_copy</th>\n",
       "      <th>CC_LMT_copy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>Masters</td>\n",
       "      <td>EXECUTIVE CONDOMINIUM</td>\n",
       "      <td>6.0</td>\n",
       "      <td>PMEB</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6896.910156</td>\n",
       "      <td>4899.080078</td>\n",
       "      <td>910.880005</td>\n",
       "      <td>...</td>\n",
       "      <td>13.233334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.233334</td>\n",
       "      <td>34500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>O-Levels</td>\n",
       "      <td>PRIVATE CONDOMINIUM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PMEB</td>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51714.781250</td>\n",
       "      <td>35740.550781</td>\n",
       "      <td>1318.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>727.629150</td>\n",
       "      <td>8530.879883</td>\n",
       "      <td>273.440002</td>\n",
       "      <td>2296.713379</td>\n",
       "      <td>27560.560547</td>\n",
       "      <td>88.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>727.629150</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>A-Levels</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PMEB</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5420.089844</td>\n",
       "      <td>5420.089844</td>\n",
       "      <td>5420.089844</td>\n",
       "      <td>...</td>\n",
       "      <td>367.389160</td>\n",
       "      <td>523.349976</td>\n",
       "      <td>122.129997</td>\n",
       "      <td>283.580841</td>\n",
       "      <td>3402.969971</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>367.389160</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_AGE     C_EDU                  C_HSE  INCM_TYP gn_occ  NUM_PRD  \\\n",
       "0     65   Masters  EXECUTIVE CONDOMINIUM       6.0   PMEB        3   \n",
       "1     86  O-Levels    PRIVATE CONDOMINIUM       2.0   PMEB        4   \n",
       "2     69  A-Levels           Not Provided       3.0   PMEB        4   \n",
       "\n",
       "   CASATD_CNT       MTHCASA       MAXCASA      MINCASA  ...      CC_AVE  \\\n",
       "0         8.0   6896.910156   4899.080078   910.880005  ...   13.233334   \n",
       "1        13.0  51714.781250  35740.550781  1318.250000  ...  727.629150   \n",
       "2         1.0   5420.089844   5420.089844  5420.089844  ...  367.389160   \n",
       "\n",
       "   MAX_MTH_TRN_AMT  MIN_MTH_TRN_AMT  AVG_TRN_AMT   ANN_TRN_AMT  ANN_N_TRX  \\\n",
       "0         0.000000         0.000000     0.000000      0.000000        0.0   \n",
       "1      8530.879883       273.440002  2296.713379  27560.560547       88.0   \n",
       "2       523.349976       122.129997   283.580841   3402.969971       78.0   \n",
       "\n",
       "    CC_LMT  C_seg  CC_AVE_copy  CC_LMT_copy  \n",
       "0  34500.0      1    13.233334      34500.0  \n",
       "1   4000.0      1   727.629150       4000.0  \n",
       "2   5000.0      1   367.389160       5000.0  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = raw_data.copy()\n",
    "\n",
    "cleaned_data = impute_missing_data(cleaned_data)\n",
    "\n",
    "cleaned_data = convert_float64_to_float32(cleaned_data)\n",
    "\n",
    "# Eliminate potential bias introduced by non-unique customer IDs.\n",
    "# Exclude the PC (Personal Computer) feature, assuming it represents a binary or categorical variation. If it corresponds to a geographical location, consider implementing geographical encoding techniques.\n",
    "cleaned_data = cleaned_data.drop(columns=[\"C_ID\", \"PC\"]).reset_index(drop=True)\n",
    "\n",
    "cleaned_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be utilizing tree-based models like XGBoost or Random Forests, where there is generally no need to standardize or normalize features. \n",
    "\n",
    "**Reasoning:**\n",
    "\n",
    "1. **Invariance to Monotonic Transformations:**\n",
    "   - Tree-based models base decisions on feature comparisons, making them insensitive to monotonic transformations like normalization or standardization.\n",
    "2. **Natural Handling of Different Scales:**\n",
    "   - These models partition the feature space based on relative feature values, not their absolute magnitude, making them robust to varying scales among features.\n",
    "\n",
    "Therefore, training the models using raw features maintains the data's original form, allowing tree-based models to effectively learn from the inherent structure and relationships within the features. Furthermore, XGBoost and Random Forests are robust machine learning models that are less sensitive to outliers, unlike clustering models like SVMs that are sensitive to boundary spaces or regression-based models.\n",
    "\n",
    "Then for categorical variables like occupation, I will be utilizing **Stratified K-Fold Target Encoding** on the specified categorical columns first.\n",
    "\n",
    "1. **Prevent Data Leakage**\n",
    "   - To prevent data leakage and ensure each fold is representative of the whole dataset given our imbalanced dataset.\n",
    "2. **Tracking Purpose**:\n",
    "   - I will keep track of all the encoders used for target encoding. This is under the assumption that my test set, or the future observation used for inferencing in production stage is from a similar distribution to my training set.\n",
    "\n",
    "**Example of loading back used encoders/scalers**\n",
    "```{python}\n",
    "encoder_config = load_catalog(\"maybank/conf/base/encoder.yaml\")\n",
    "\n",
    "for col, encoder_file in config[\"encoders\"].items():\n",
    "    encoder = joblib.load(encoder_file)\n",
    "    encoded_df[col] = encoder.transform(encoded_df[col])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to maybank/data/preprocessed/processed_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_AGE</th>\n",
       "      <th>NUM_PRD</th>\n",
       "      <th>CASATD_CNT</th>\n",
       "      <th>MTHCASA</th>\n",
       "      <th>MAXCASA</th>\n",
       "      <th>MINCASA</th>\n",
       "      <th>DRvCR</th>\n",
       "      <th>MTHTD</th>\n",
       "      <th>MAXTD</th>\n",
       "      <th>Asset value</th>\n",
       "      <th>...</th>\n",
       "      <th>CC_LMT</th>\n",
       "      <th>C_seg</th>\n",
       "      <th>OWN_CASA</th>\n",
       "      <th>OWN_TD</th>\n",
       "      <th>OWN_CC</th>\n",
       "      <th>OWN_PREV_CC</th>\n",
       "      <th>INCM_TYP_encoded</th>\n",
       "      <th>C_EDU_encoded</th>\n",
       "      <th>C_HSE_encoded</th>\n",
       "      <th>gn_occ_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6896.910156</td>\n",
       "      <td>4899.080078</td>\n",
       "      <td>910.880005</td>\n",
       "      <td>1.020768e+06</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>25000.00000</td>\n",
       "      <td>111896.906250</td>\n",
       "      <td>...</td>\n",
       "      <td>34500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.155051</td>\n",
       "      <td>0.181734</td>\n",
       "      <td>0.226390</td>\n",
       "      <td>0.151867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51714.781250</td>\n",
       "      <td>35740.550781</td>\n",
       "      <td>1318.250000</td>\n",
       "      <td>8.326420e+00</td>\n",
       "      <td>575572.0</td>\n",
       "      <td>135026.15625</td>\n",
       "      <td>627286.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.178821</td>\n",
       "      <td>0.165399</td>\n",
       "      <td>0.182339</td>\n",
       "      <td>0.152018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5420.089844</td>\n",
       "      <td>5420.089844</td>\n",
       "      <td>5420.089844</td>\n",
       "      <td>4.106600e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>64161.738281</td>\n",
       "      <td>...</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.132776</td>\n",
       "      <td>0.199774</td>\n",
       "      <td>0.172529</td>\n",
       "      <td>0.152039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_AGE  NUM_PRD  CASATD_CNT       MTHCASA       MAXCASA      MINCASA  \\\n",
       "0     65        3         8.0   6896.910156   4899.080078   910.880005   \n",
       "1     86        4        13.0  51714.781250  35740.550781  1318.250000   \n",
       "2     69        4         1.0   5420.089844   5420.089844  5420.089844   \n",
       "\n",
       "          DRvCR     MTHTD         MAXTD    Asset value  ...   CC_LMT  C_seg  \\\n",
       "0  1.020768e+06  105000.0   25000.00000  111896.906250  ...  34500.0      1   \n",
       "1  8.326420e+00  575572.0  135026.15625  627286.750000  ...   4000.0      1   \n",
       "2  4.106600e-01       0.0       0.00000   64161.738281  ...   5000.0      1   \n",
       "\n",
       "   OWN_CASA  OWN_TD  OWN_CC  OWN_PREV_CC  INCM_TYP_encoded  C_EDU_encoded  \\\n",
       "0         1       1       1            1          0.155051       0.181734   \n",
       "1         1       1       1            1          0.178821       0.165399   \n",
       "2         1       0       1            1          0.132776       0.199774   \n",
       "\n",
       "   C_HSE_encoded  gn_occ_encoded  \n",
       "0       0.226390        0.151867  \n",
       "1       0.182339        0.152018  \n",
       "2       0.172529        0.152039  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maybank.src.feature_engineering.feature_engineering import (\n",
    "    add_features,\n",
    "    stratified_kfold_target_encoding,\n",
    "    standardize_columns,\n",
    "    # Although not utilized, StandardScaler was tested out.\n",
    "    # This was initially implemented due to the initial approach of using clustering.\n",
    ")\n",
    "from maybank.src.utils.utils import save_processed_data_as_csv\n",
    "\n",
    "processed_data = cleaned_data.copy()\n",
    "\n",
    "processed_data = add_features(processed_data)\n",
    "\n",
    "processed_data = stratified_kfold_target_encoding(\n",
    "    processed_data,\n",
    "    [\"INCM_TYP\", \"C_EDU\", \"C_HSE\", \"gn_occ\"],\n",
    "    \"C_seg\",\n",
    "    encoder_folder=\"maybank/data/models/\",\n",
    ")\n",
    "\n",
    "save_processed_data_as_csv(processed_data, \"maybank/data/preprocessed/processed_data.csv\")\n",
    "\n",
    "processed_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please open `maybank/data/preprocessed/EDA.html` to view the interactive html EDA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:02 -> (00:00 left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report maybank/data/preprocessed/EDA.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "perform_eda_with_sweetviz(\n",
    "    processed_data,\n",
    "    target_feat=\"C_seg\",\n",
    "    html_file_path=\"maybank/data/preprocessed/EDA.html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sampling   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to incorporate a combined under-sampling and over-sampling strategy to ensure a more balanced training fold for each iteration. Below are some common methods from [imbalanced-learn](https://imbalanced-learn.org/stable/index.html).\n",
    "\n",
    "**Under-sampling methods:**\n",
    "\n",
    "| Method                      | Advantages                               | Disadvantages                              |\n",
    "|-----------------------------|------------------------------------------|--------------------------------------------|\n",
    "| ClusterCentroids            | Preserves information while reducing majority class. | May not accurately capture underlying distribution. |\n",
    "| CondensedNearestNeighbour   | Selects subset representing majority class. | May inadvertently remove informative instances.         |\n",
    "| EditedNearestNeighbours     | Removes noisy majority class instances.  | Sensitive to noise, may not fully address imbalance. |\n",
    "| RandomUnderSampler          | Simple and fast.                         | May discard useful instances, effectiveness in solving imbalance may vary. |\n",
    "\n",
    "Considering that we have a large majority class, I will opt for the fast and straightforward **RandomUnderSampler**.\n",
    "\n",
    "**Over-sampling methods:**\n",
    "\n",
    "| Method                      | Advantages                               | Disadvantages                              |\n",
    "|-----------------------------|------------------------------------------|--------------------------------------------|\n",
    "| RandomOverSampler           | Simple and effective.                    | May lead to overfitting, reduction in diversity. |\n",
    "| SMOTENC                     | Handles both numerical and categorical features. | Requires parameter tuning, can be computationally intensive. |\n",
    "| ADASYN                      | Focuses on low-density regions.          | Sensitive to noise, requires careful parameter adjustment. |\n",
    "\n",
    "Since our aim is to uncover hidden affluent customers, I prefer a more intricate approach for over-sampling.\n",
    "\n",
    "Comparing **SMOTENC** with **ADASYN**, **ADASYN** appears preferable due to its capability to generate synthetic samples in regions where the classifier is likely to make errors. **ADASYN** treats variables as continuous scale. On the other hand, **SMOTENC** is tailored for datasets with a mix of categorical and continuous features, incorporating the nature of categorical features during synthetic sample generation.\n",
    "\n",
    "Therefore, I will utilize **SMOTENC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 55157, 1: 55157})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_AGE</th>\n",
       "      <th>NUM_PRD</th>\n",
       "      <th>CASATD_CNT</th>\n",
       "      <th>MTHCASA</th>\n",
       "      <th>MAXCASA</th>\n",
       "      <th>MINCASA</th>\n",
       "      <th>DRvCR</th>\n",
       "      <th>MTHTD</th>\n",
       "      <th>MAXTD</th>\n",
       "      <th>Asset value</th>\n",
       "      <th>...</th>\n",
       "      <th>ANN_N_TRX</th>\n",
       "      <th>CC_LMT</th>\n",
       "      <th>OWN_CASA</th>\n",
       "      <th>OWN_TD</th>\n",
       "      <th>OWN_CC</th>\n",
       "      <th>OWN_PREV_CC</th>\n",
       "      <th>INCM_TYP_encoded</th>\n",
       "      <th>C_EDU_encoded</th>\n",
       "      <th>C_HSE_encoded</th>\n",
       "      <th>gn_occ_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55695</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.180085</td>\n",
       "      <td>0.165917</td>\n",
       "      <td>0.137042</td>\n",
       "      <td>0.152039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30952</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.130531</td>\n",
       "      <td>0.199774</td>\n",
       "      <td>0.172529</td>\n",
       "      <td>0.152039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63353</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19358.650391</td>\n",
       "      <td>19358.650391</td>\n",
       "      <td>19358.650391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19358.650391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178664</td>\n",
       "      <td>0.168420</td>\n",
       "      <td>0.171835</td>\n",
       "      <td>0.140593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       C_AGE  NUM_PRD  CASATD_CNT       MTHCASA       MAXCASA       MINCASA  \\\n",
       "55695     46        1         0.0      0.000000      0.000000      0.000000   \n",
       "30952     48        1         0.0      0.000000      0.000000      0.000000   \n",
       "63353     79        1         1.0  19358.650391  19358.650391  19358.650391   \n",
       "\n",
       "       DRvCR  MTHTD  MAXTD   Asset value  ...  ANN_N_TRX  CC_LMT  OWN_CASA  \\\n",
       "55695    0.0    0.0    0.0      0.000000  ...        0.0     0.0         0   \n",
       "30952    0.0    0.0    0.0      0.000000  ...        0.0     0.0         0   \n",
       "63353    0.0    0.0    0.0  19358.650391  ...        0.0     0.0         1   \n",
       "\n",
       "       OWN_TD  OWN_CC  OWN_PREV_CC  INCM_TYP_encoded  C_EDU_encoded  \\\n",
       "55695       0       0            0          0.180085       0.165917   \n",
       "30952       0       0            0          0.130531       0.199774   \n",
       "63353       0       0            0          0.178664       0.168420   \n",
       "\n",
       "       C_HSE_encoded  gn_occ_encoded  \n",
       "55695       0.137042        0.152039  \n",
       "30952       0.172529        0.152039  \n",
       "63353       0.171835        0.140593  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maybank.src.data_processing.data_sampling import resample_data\n",
    "from collections import Counter\n",
    "\n",
    "# For testing purposes\n",
    "resampled_X, resampled_y = resample_data(\n",
    "    processed_data,\n",
    "    \"C_seg\",\n",
    "    [\n",
    "        \"HL_tag\",\n",
    "        \"AL_tag\",\n",
    "        \"OWN_CASA\",\n",
    "        \"OWN_TD\",\n",
    "        \"OWN_CC\",\n",
    "        \"OWN_PREV_CC\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(Counter(resampled_y))\n",
    "\n",
    "resampled_X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing 2 common classification approaches, evaluated on average recall and precision using (10 folds) k-fold cross-validation to determine the best approach to proceed with:\n",
    "\n",
    "- **Random Forest**\n",
    "  - **n_estimators:** 100\n",
    "  \n",
    "- **XGBoost Classifier**\n",
    "  - **objective:** binary: logistic\n",
    "  - **learning_rate:** 0.001\n",
    "  - **n_estimators:** 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [05:00, 30.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Avg Precision: 0.58\n",
      "Random Forest - Avg Recall: 0.63\n",
      "Random Forest - Avg F1 Score: 0.6\n",
      "XGBoost - Avg Precision: 0.48\n",
      "XGBoost - Avg Recall: 0.78\n",
      "XGBoost - Avg F1 Score: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from maybank.src.modeling.modeling import perform_cross_validation_over_models\n",
    "\n",
    "perform_cross_validation_over_models(\n",
    "    processed_data,\n",
    "    label_column=\"C_seg\",\n",
    "    binary_columns=[\"HL_tag\", \"AL_tag\", \"OWN_CASA\", \"OWN_TD\", \"OWN_CC\", \"OWN_PREV_CC\"],\n",
    "    number_folds=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building XGBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it would seem that **XGBoost** has a `highest average recall score` which is our objective, to have high classification of affluent classes. Then, any false positives will actually be our targeted potential affluent customers.\n",
    "\n",
    "I will be using **Optuna**, an optimization library, to automatically find the best hyperparameters for an XGBoost classifier, ensuring that the model I train is as effective as possible for the given dataset.\n",
    "\n",
    "By specifying a range of values for each parameter, **Optuna** iteratively tests different combinations using a smart search strategy, assessing their performance based on recall score through k-fold cross-validation. This method helps me identify the optimal configuration that maximizes recall, which is crucial for reducing false negatives in my predictions.\n",
    "\n",
    "Additionally, I have actually already implemented the **SelectFromModel** feature from scikit-learn, which serves as a meta-transformer for selecting the best features based on importance weights.\n",
    "\n",
    "After finding the best hyperparmeters and training the XGBoost model, **SelectFromModel** reviews the model's feature importances and retains only those that meet a specified threshold. However since the number of features is already so small, I have commented it out. However feel free to uncomment it out to rerun, it will work smoothly under `tune_xgboost_model()` in `src/modeling/modeling.py`.\n",
    "\n",
    "This process not only helps in enhancing the model's performance by focusing on the most relevant features but also aids in reducing complexity and improving interpretability. The integration of feature selection within the cross-validation loop ensures that the feature selection process is robust and prevents overfitting, making my model more generalizable to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 14:09:02,817] A new study created in memory with name: no-name-ec01eb00-646d-4c24-92a8-7b2879e739c9\n",
      "[I 2024-03-31 14:10:20,491] Trial 0 finished with value: 0.5326139540221855 and parameters: {'n_estimators': 113, 'max_depth': 10, 'learning_rate': 0.3294209024773819, 'subsample': 0.7334155879588973, 'colsample_bytree': 0.9228784287980911, 'gamma': 3.260515446607697, 'min_child_weight': 10}. Best is trial 0 with value: 0.5326139540221855.\n",
      "[I 2024-03-31 14:11:51,435] Trial 1 finished with value: 0.5400248453343304 and parameters: {'n_estimators': 255, 'max_depth': 10, 'learning_rate': 0.12184039324682568, 'subsample': 0.7959468912328407, 'colsample_bytree': 0.92544620412886, 'gamma': 1.7938336175218594, 'min_child_weight': 5}. Best is trial 1 with value: 0.5400248453343304.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Maximum Recall: {'n_estimators': 255, 'max_depth': 10, 'learning_rate': 0.12184039324682568, 'subsample': 0.7959468912328407, 'colsample_bytree': 0.92544620412886, 'gamma': 1.7938336175218594, 'min_child_weight': 5}\n",
      "Model saved to maybank/data/models/xgboost_model_20240331_141152.pkl\n"
     ]
    }
   ],
   "source": [
    "from maybank.src.modeling.modeling import tune_xgboost_model\n",
    "\n",
    "# Testing function out, hence only 2 trials\n",
    "# main_pipeline.py will be running it fully with parameters loaded from parameters.yaml\n",
    "tune_xgboost_model(\n",
    "    processed_data,\n",
    "    label_column=\"C_seg\",\n",
    "    binary_columns=[\"HL_tag\", \"AL_tag\", \"OWN_CASA\", \"OWN_TD\", \"OWN_CC\", \"OWN_PREV_CC\"],\n",
    "    n_trials=2, # For optuna number of trials\n",
    "    n_splits=10, # For k_fold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
